{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis - Stock Price Prediction\n",
    "## 34-Year Historical Market Data (1990-2024)\n",
    "\n",
    "This notebook performs comprehensive data exploration including:\n",
    "- Data quality assessment\n",
    "- Temporal analysis and trend visualization\n",
    "- Statistical profiling\n",
    "- Feature relationship analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['font.size'] = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Initial Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('../stock_data.csv', parse_dates=['dt'])\n",
    "df.set_index('dt', inplace=True)\n",
    "\n",
    "print(f\"Dataset Shape: {df.shape}\")\n",
    "print(f\"\\nDate Range: {df.index.min()} to {df.index.max()}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Data types and memory usage\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Missing values analysis\n",
    "missing_data = pd.DataFrame({\n",
    "    'Missing_Count': df.isnull().sum(),\n",
    "    'Missing_Percentage': (df.isnull().sum() / len(df)) * 100\n",
    "}).sort_values('Missing_Count', ascending=False)\n",
    "\n",
    "print(\"Missing Values Summary:\")\n",
    "print(missing_data[missing_data['Missing_Count'] > 0])\n",
    "\n",
    "# Visualize missing data\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(df.isnull(), cbar=True, yticklabels=False, cmap='viridis')\n",
    "plt.title('Missing Data Heatmap')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check for date continuity and gaps\n",
    "date_diff = df.index.to_series().diff()\n",
    "gaps = date_diff[date_diff > pd.Timedelta(days=3)]  # Gaps > 3 days (excluding weekends)\n",
    "\n",
    "print(f\"\\nNumber of significant gaps (>3 days): {len(gaps)}\")\n",
    "if len(gaps) > 0:\n",
    "    print(\"\\nTop 10 largest gaps:\")\n",
    "    print(gaps.nlargest(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Detect negative or impossible values\n",
    "print(\"Data Range Validation:\")\n",
    "print(f\"\\nNegative prices in sp500: {(df['sp500'] < 0).sum()}\")\n",
    "print(f\"Negative volumes in sp500_volume: {(df['sp500_volume'] < 0).sum()}\")\n",
    "print(f\"Negative VIX: {(df['vix'] < 0).sum()}\")\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\nBasic Statistics:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Temporal Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot S&P 500 historical trends with major events\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "ax.plot(df.index, df['sp500'], linewidth=1.5, color='steelblue', label='S&P 500')\n",
    "\n",
    "# Annotate major market events\n",
    "events = [\n",
    "    ('2000-03-10', 'Dot-com Crash', 'red'),\n",
    "    ('2008-09-15', '2008 Financial Crisis', 'red'),\n",
    "    ('2020-03-11', 'COVID-19 Pandemic', 'red'),\n",
    "    ('2009-03-09', 'Market Bottom 2009', 'green'),\n",
    "    ('2020-03-23', 'COVID Bottom', 'green')\n",
    "]\n",
    "\n",
    "for date, label, color in events:\n",
    "    if pd.Timestamp(date) in df.index:\n",
    "        ax.axvline(pd.Timestamp(date), color=color, linestyle='--', alpha=0.7, linewidth=1)\n",
    "        ax.text(pd.Timestamp(date), ax.get_ylim()[1]*0.95, label, \n",
    "                rotation=90, verticalalignment='top', fontsize=9)\n",
    "\n",
    "ax.set_xlabel('Date', fontsize=12)\n",
    "ax.set_ylabel('S&P 500 Price', fontsize=12)\n",
    "ax.set_title('S&P 500 Historical Price (1990-2024) with Major Market Events', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Stationarity test (Augmented Dickey-Fuller)\n",
    "def adf_test(series, name):\n",
    "    result = adfuller(series.dropna())\n",
    "    print(f\"\\nADF Test for {name}:\")\n",
    "    print(f\"  ADF Statistic: {result[0]:.6f}\")\n",
    "    print(f\"  p-value: {result[1]:.6f}\")\n",
    "    print(f\"  Critical Values:\")\n",
    "    for key, value in result[4].items():\n",
    "        print(f\"    {key}: {value:.3f}\")\n",
    "    if result[1] <= 0.05:\n",
    "        print(f\"  Result: STATIONARY (reject H0)\")\n",
    "    else:\n",
    "        print(f\"  Result: NON-STATIONARY (fail to reject H0)\")\n",
    "\n",
    "adf_test(df['sp500'], 'S&P 500')\n",
    "adf_test(df['vix'], 'VIX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate returns for stationarity\n",
    "df['sp500_returns'] = df['sp500'].pct_change()\n",
    "adf_test(df['sp500_returns'].dropna(), 'S&P 500 Returns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Volatility clustering visualization\n",
    "fig, axes = plt.subplots(2, 1, figsize=(16, 10))\n",
    "\n",
    "# S&P 500 Returns\n",
    "axes[0].plot(df.index, df['sp500_returns'], linewidth=0.5, color='navy', alpha=0.7)\n",
    "axes[0].set_title('S&P 500 Daily Returns - Volatility Clustering', fontsize=13, fontweight='bold')\n",
    "axes[0].set_ylabel('Returns')\n",
    "axes[0].axhline(0, color='red', linestyle='--', alpha=0.5)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# VIX\n",
    "axes[1].plot(df.index, df['vix'], linewidth=1, color='darkred', alpha=0.8)\n",
    "axes[1].set_title('VIX (Fear Index) Over Time', fontsize=13, fontweight='bold')\n",
    "axes[1].set_xlabel('Date')\n",
    "axes[1].set_ylabel('VIX')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Statistical Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Descriptive statistics with skewness and kurtosis\n",
    "stats_df = df.describe().T\n",
    "stats_df['skewness'] = df.skew()\n",
    "stats_df['kurtosis'] = df.kurtosis()\n",
    "\n",
    "print(\"Extended Descriptive Statistics:\")\n",
    "stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Distribution plots for key features\n",
    "features_to_plot = ['sp500', 'vix', 'sp500_volume', 'us3m', 'epu', 'GPRD']\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(15, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, col in enumerate(features_to_plot):\n",
    "    axes[idx].hist(df[col].dropna(), bins=50, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "    axes[idx].set_title(f'Distribution of {col}', fontweight='bold')\n",
    "    axes[idx].set_xlabel(col)\n",
    "    axes[idx].set_ylabel('Frequency')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add normal distribution overlay\n",
    "    mu, sigma = df[col].mean(), df[col].std()\n",
    "    x = np.linspace(df[col].min(), df[col].max(), 100)\n",
    "    axes[idx].plot(x, stats.norm.pdf(x, mu, sigma) * len(df[col].dropna()) * (df[col].max() - df[col].min()) / 50, \n",
    "                   'r-', linewidth=2, label='Normal Dist')\n",
    "    axes[idx].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Correlation analysis\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "# Heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            center=0, square=True, linewidths=1)\n",
    "plt.title('Pearson Correlation Matrix', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Top correlations with S&P 500\n",
    "sp500_corr = correlation_matrix['sp500'].sort_values(ascending=False)\n",
    "print(\"\\nTop Correlations with S&P 500:\")\n",
    "print(sp500_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Spearman correlation (non-linear relationships)\n",
    "spearman_corr = df.corr(method='spearman')\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(spearman_corr, annot=True, fmt='.2f', cmap='viridis', \n",
    "            center=0, square=True, linewidths=1)\n",
    "plt.title('Spearman Correlation Matrix (Rank-based)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Multicollinearity check using VIF\n",
    "# Select numeric columns excluding target\n",
    "features_for_vif = ['vix', 'sp500_volume', 'djia', 'djia_volume', 'hsi', \n",
    "                    'ads', 'us3m', 'joblessness', 'epu', 'GPRD', 'prev_day']\n",
    "\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data['Feature'] = features_for_vif\n",
    "vif_data['VIF'] = [variance_inflation_factor(df[features_for_vif].dropna().values, i) \n",
    "                   for i in range(len(features_for_vif))]\n",
    "vif_data = vif_data.sort_values('VIF', ascending=False)\n",
    "\n",
    "print(\"\\nVariance Inflation Factor (VIF):\")\n",
    "print(\"Note: VIF > 10 indicates high multicollinearity\")\n",
    "print(vif_data)\n",
    "\n",
    "# Visualize VIF\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(vif_data['Feature'], vif_data['VIF'], color='coral')\n",
    "plt.axvline(x=10, color='red', linestyle='--', label='VIF=10 threshold')\n",
    "plt.xlabel('VIF Score', fontsize=12)\n",
    "plt.title('Variance Inflation Factor by Feature', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Relationship Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# VIX vs S&P 500 relationship (typically inverse)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(df['sp500'], df['vix'], alpha=0.3, s=10, c=df.index.year, cmap='viridis')\n",
    "plt.colorbar(label='Year')\n",
    "plt.xlabel('S&P 500', fontsize=12)\n",
    "plt.ylabel('VIX (Volatility Index)', fontsize=12)\n",
    "plt.title('VIX vs S&P 500: Inverse Relationship Analysis', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Correlation between VIX and S&P 500: {df['vix'].corr(df['sp500']):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Volume analysis on high volatility days\n",
    "df['abs_returns'] = df['sp500_returns'].abs()\n",
    "high_vol_days = df['abs_returns'] > df['abs_returns'].quantile(0.95)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Volume comparison\n",
    "axes[0].boxplot([df.loc[~high_vol_days, 'sp500_volume'].dropna(), \n",
    "                 df.loc[high_vol_days, 'sp500_volume'].dropna()],\n",
    "                labels=['Normal Days', 'High Volatility Days'])\n",
    "axes[0].set_ylabel('Volume', fontsize=12)\n",
    "axes[0].set_title('Volume on Normal vs High Volatility Days', fontsize=13, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# VIX comparison\n",
    "axes[1].boxplot([df.loc[~high_vol_days, 'vix'].dropna(), \n",
    "                 df.loc[high_vol_days, 'vix'].dropna()],\n",
    "                labels=['Normal Days', 'High Volatility Days'])\n",
    "axes[1].set_ylabel('VIX', fontsize=12)\n",
    "axes[1].set_title('VIX on Normal vs High Volatility Days', fontsize=13, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Macroeconomic indicators impact\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Unemployment vs S&P 500\n",
    "axes[0, 0].scatter(df['joblessness'], df['sp500'], alpha=0.5, s=10)\n",
    "axes[0, 0].set_xlabel('Joblessness Quartile')\n",
    "axes[0, 0].set_ylabel('S&P 500')\n",
    "axes[0, 0].set_title('Unemployment vs Market Performance')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Interest rates vs S&P 500\n",
    "axes[0, 1].scatter(df['us3m'], df['sp500'], alpha=0.5, s=10, c='green')\n",
    "axes[0, 1].set_xlabel('US 3-Month Treasury Yield')\n",
    "axes[0, 1].set_ylabel('S&P 500')\n",
    "axes[0, 1].set_title('Interest Rates vs Market Performance')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# EPU vs S&P 500\n",
    "axes[1, 0].scatter(df['epu'], df['sp500'], alpha=0.5, s=10, c='orange')\n",
    "axes[1, 0].set_xlabel('Economic Policy Uncertainty')\n",
    "axes[1, 0].set_ylabel('S&P 500')\n",
    "axes[1, 0].set_title('Policy Uncertainty vs Market Performance')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# GPRD vs S&P 500\n",
    "axes[1, 1].scatter(df['GPRD'], df['sp500'], alpha=0.5, s=10, c='red')\n",
    "axes[1, 1].set_xlabel('Geopolitical Risk')\n",
    "axes[1, 1].set_ylabel('S&P 500')\n",
    "axes[1, 1].set_title('Geopolitical Risk vs Market Performance')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cross-market correlation analysis\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# DJIA vs S&P 500\n",
    "axes[0].scatter(df['djia'], df['sp500'], alpha=0.4, s=10)\n",
    "axes[0].set_xlabel('DJIA', fontsize=12)\n",
    "axes[0].set_ylabel('S&P 500', fontsize=12)\n",
    "axes[0].set_title(f'DJIA vs S&P 500 (Corr: {df[\"djia\"].corr(df[\"sp500\"]):.4f})', fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# HSI vs S&P 500\n",
    "axes[1].scatter(df['hsi'], df['sp500'], alpha=0.4, s=10, c='green')\n",
    "axes[1].set_xlabel('Hang Seng Index', fontsize=12)\n",
    "axes[1].set_ylabel('S&P 500', fontsize=12)\n",
    "axes[1].set_title(f'HSI vs S&P 500 (Corr: {df[\"hsi\"].corr(df[\"sp500\"]):.4f})', fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Key Findings Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"=\"*80)\n",
    "print(\"EDA KEY FINDINGS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. DATA QUALITY:\")\n",
    "print(f\"   - Total observations: {len(df):,}\")\n",
    "print(f\"   - Date range: {df.index.min().strftime('%Y-%m-%d')} to {df.index.max().strftime('%Y-%m-%d')}\")\n",
    "print(f\"   - Missing values: {df.isnull().sum().sum()} total\")\n",
    "print(f\"   - No negative prices or volumes detected\")\n",
    "\n",
    "print(\"\\n2. STATIONARITY:\")\n",
    "print(f\"   - S&P 500 price series is NON-STATIONARY (expected for price data)\")\n",
    "print(f\"   - S&P 500 returns are STATIONARY (suitable for modeling)\")\n",
    "print(f\"   - Differencing/returns transformation recommended for models\")\n",
    "\n",
    "print(\"\\n3. CORRELATIONS:\")\n",
    "print(f\"   - VIX vs S&P 500: {df['vix'].corr(df['sp500']):.4f} (inverse relationship confirmed)\")\n",
    "print(f\"   - DJIA vs S&P 500: {df['djia'].corr(df['sp500']):.4f} (very high correlation)\")\n",
    "print(f\"   - HSI vs S&P 500: {df['hsi'].corr(df['sp500']):.4f} (moderate international correlation)\")\n",
    "\n",
    "print(\"\\n4. MULTICOLLINEARITY:\")\n",
    "high_vif = vif_data[vif_data['VIF'] > 10]\n",
    "if len(high_vif) > 0:\n",
    "    print(f\"   - {len(high_vif)} features have VIF > 10:\")\n",
    "    for _, row in high_vif.iterrows():\n",
    "        print(f\"     * {row['Feature']}: {row['VIF']:.2f}\")\n",
    "else:\n",
    "    print(f\"   - No severe multicollinearity detected (all VIF < 10)\")\n",
    "\n",
    "print(\"\\n5. VOLATILITY:\")\n",
    "print(f\"   - Volatility clustering observed in returns\")\n",
    "print(f\"   - VIX spikes during major crises (2008, 2020)\")\n",
    "print(f\"   - Higher volume on high volatility days confirmed\")\n",
    "\n",
    "print(\"\\n6. RECOMMENDATIONS FOR MODELING:\")\n",
    "print(f\"   - Use returns instead of raw prices for stationarity\")\n",
    "print(f\"   - Consider feature selection to address multicollinearity\")\n",
    "print(f\"   - Implement temporal train/test split (NO shuffling)\")\n",
    "print(f\"   - Account for volatility clustering in model selection\")\n",
    "print(f\"   - Crisis periods (2008, 2020) may need special handling\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Save cleaned data for next phase\n",
    "df.to_csv('../data/processed/eda_cleaned.csv')\n",
    "print(\"\\nCleaned data saved to: data/processed/eda_cleaned.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
